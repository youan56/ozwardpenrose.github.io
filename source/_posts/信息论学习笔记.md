---
title: 信息论学习笔记
toc: true
abbrlink: 34190
date: 2020-04-11 07:56:30
tags: -笔记
categories: 专业课程
---

## 课程背景介绍

信息论是在信息可以量度的基础上，研究有效地和可靠地传递信息的科学，它涉及信息量度、信息特性、信息传输速率、信道容量、干扰对信息传输的影响等方面的知识。通常把上述范围的信息论称为狭义的信息论，又因为它的创始人是香农，故又称为香农信息论。

广义信息论则包含通信的全部统计问题的研究，除了香农信息论之外，还包括信号设计、噪声理论、信号的检测与估值等。当信息在传输、存储和处理的过程中，不可避免地要受到噪声或其它无用信号的干扰，信息理论就是为能可靠地有效地从数据中提取信息，提供必要的数据和方法。

信息在早些时期的定义是由奈奎斯特（Nyquist,H.）和哈特莱（Hartley,L.V.R.）在20世纪20年代提出来的。1924年奈奎斯特解释了信号带宽和信息速率之间的关系；1928年哈莱特最早研究了通信系统传输信息的能力，给出了信息度量；1936年阿姆斯特朗（Armstrong）提出了增大带宽可以使抗干扰能力加强。这些工作都给香农很大的影响，他在1941~1944年对通信和密码进行深入研究，用概率论的方法研究通信系统，揭示了通信系统传递的对象就是信息，并对信息给以科学的定量描述，给出了信息熵的概念。指出通信系统的中心问题是在噪声下如何有效而可靠地传送信息以及实现这一目标的主要方法是编码等。这一成果于1948年以《通信的数学理论》（A mathematical theory of communication）为题公开发表。香农因此成为信息论的奠基人。

50年代信息论在学术界引起了巨大的反响。1951年美国IRE成立了信息论组，并于1955年正式出版了信息论汇刊。60年代信道编码技术有较大进展，使它成为信息论的又一重要分支。70年代，有关信息论的研究，从点与点间的单用户通信推广到多用户系统的研究。1972年盖弗（Cover）发表了有关广播信道的研究，以后陆续有关于多接入信道和广播信道模型的研究，但由于这些问题比较难，到目前为止，多用户信息论研究得不多，还有许多尚待解决的课题。

| 年份   | 事件                                                         |
| :----- | :----------------------------------------------------------- |
| 1948年 | 香农对信息给以科学的定量描述，给出了信息熵的概念。           |
| 1949年 | Kraft-McMillan不等式为给定码字长度集合提供前缀码的必要充分的条件 |
| 1956年 | 麦克米伦发现了Kraft-McMillan不等式，证明了唯一可译码代码的普遍性 |
| 1967年 | Elwyn Berlekamp发明了解码算法：Bose-Chaudhuri-Hocquenghem（BCH） |
| 2002年 | Stolte首次提出极化码的编码方案                               |

 信息论是电子信息大类专业的核心课程之一，具有重要的实践意义。

本笔记的参考教材为陈运的《信息论与编码》。

## 信息论绪论

信息论的研究对象主要是信息传输系统，其模型：

![UTOOLS1587180721698.png](http://yanxuan.nosdn.127.net/bdff98ffeb66bb80e47bc1ea4c490c37.png)

信息传输系统的要求：可靠性、有效性、保密性、认证性。

**通信系统的简化模型：信源和信宿**

![UTOOLS1587263192881.png](http://yanxuan.nosdn.127.net/d50423314a3b4c49f5fcdcf47864e251.png)

* **信源**: **产生消息的源。消息可以是文字、语言、图像等。**

**它可以是离散序列，也可以是连续形式，但都是随机发生的，亦即在未收到这些消息之前不可能确切地知道它们的内容。这些消息可以用随机变量或随机过程来描述。**

**信源研究的主要内容是消息的统计特性和信源产生信息的速率。**

* **信宿：信息传送过程中的接收者，即接收消息的人或物。信宿和信源可处于不同的地点或存在于不同时刻。**

* **干扰源：在信道中引入噪声和干扰，这是一种等效的表达方式。为了分析方便，把在系统中其他各部分产生的噪声和干扰都等效成信道干扰，并集中作用于信道。**

  **由于噪声和干扰往往具有随机性，因此它是划分信道的重要因素，并且是决定信道传输能力的决定因素。**

  **实际干扰可以分成以下两大类：**

  1、加性干扰。由外界引入的随机干扰，如天电干扰以及设备内部噪声，它们与信道的输入信号统计无关。信道的输出是输入信号和干扰的和。**

  2、乘性干扰。信号在传播过程中由于物理条件的变化引起信号参量的随机变化而构成的干扰。此时信道的输出信号是输入信号与某些随机参量相乘的结果。**

  **研究信道的中心课题是它的统计特性和传输能力。**

* **编码**：信源编码、纠错编码、调制器

* **译码**：信源编码器和信道编码器（纠错译码器和解调器）

**信息论的研究内容**：狭义相对论（香农基本理论），主要研究信息测度、信道容量及信源和信道编码理论；一般信息论（通信理论），主要研究信息传输和处理的问题。

## 信源熵

信息论是在信息可以度量的前提下，研究有效地可靠地安全的传递信息的科学，信息的可度量性是建立信息论的基础。信息度械的方法有：结构度量、统计度量、语义度量、语用度量、模糊度量等。最常用的方法是统计度量。它用事件统计发生概率的对数来描述事物的不确定性，得到消息的信息量，进而建立墒的概念。墒的概念是香农信息论最基本、最重要的概念。

信源发出消息，消息载荷信息，而消息又具有不确定性，所以可用随机变量或随机矢量来
描述信源输出的消息，或者说用概率空间来描述信源。

一类信源输出的消息常常以一个个符号的形式出现，例如文字、字母等，这些符号的取值
是有限的或可数的，这样的信源称为离散信源。有的离散信源只涉及一个随机事件，有的离散信源涉及多个随机事件，分别称为单符号离散信源和多符号离散信源，可分别用离散随机变量和随机矢量来描述。另一类输出连续消息的信源称为连续信源，可用随机过程来描述。

### 单符号离散信源的数学模型

对千离散随机变量X, 取值于集合{a1, a2, ···, ai, ···, an}
其中n 可以是有限正整数，也可以是可数无限大整数，即nE /(整数域）， X属于{a;,i=l,2, …， n} 。规定集合中各个元素的概率为p(ai), 即p(ai) = P(X = ai)。单符号离散信源的数学模型可表示为：

![UTOOLS1587346191081.png](http://yanxuan.nosdn.127.net/6bc1436255be1100fc3cb6c6f3e62460.png)

还要关注一下无条件概率、条件概率和联合概率。他们的性质和计算如下：

![UTOOLS1587373599886.png](http://yanxuan.nosdn.127.net/7b5b2cabd0ad687c5c41485e790d408e.png)

![UTOOLS1587373627238.png](http://yanxuan.nosdn.127.net/3204ec9a78e90a3a2e9f9ecca24517f2.png)

条件概率：P（A|B）=P（AB）/P（B）表示B已发生的情况下A发生的概率。

### 自信息量和信源熵

从信息源获取信息的过程就是其不确定性缩减的过程，随机事件包含的**信息**与其不确定性紧密相关，在统计分析中，使用**概率**作为衡量不确定性的一种指标，可以推论出：**随机事件包含信息的度量应是其概率的函数**。小概率事件发生时得到的信息量大，而大概率事件发生时得到的信息量小（ 信息量是概率的降函数）。相互独立的事件（积事件）同时发生时得到的信息量应是各个事件单独发生时得到的信息量之和

  **自信息量的定义:任意随机事件的自信息量定义为该事件发生概率的对数的负值**。对应的公式如下：
$$
I\left(x_{i}\right)=\log \left[\frac{1}{p\left(x_{i}\right)}\right]=-\log p\left(x_{i}\right)
$$
自信息量的单位取决于对数选取的底，当对数的底取2的时候，单位为bit；以自然底数e为为底时，单位为奈特nat；n当以10为底时，单位为哈特hart。他们的换算关系：

![UTOOLS1587373762720.png](http://yanxuan.nosdn.127.net/056cd6a9e398998eb43bc2505be25b3a.png)

需要注意的是，信息星是纯数，信息量单位只是为了标示不同底数的对数值，并没有量纲
的含义。

自信息量的性质：非负；p=1时，I=0；p=0时，I=∞；I是p的单调递减函数。

**联合自信息量：考虑两个随机事件的离散信源，定义二维联合集XY上的元素（xi yj  ）的联合自信息量定义为：**
$$
I\left(x_{i} y_{j}\right)=-\log p\left(x_{i} y_{j}\right)
$$
式中xi yj 为积事件；p（xi yj）为元素   的二维联合概率，当X和Y相互独立时有：
$$
I\left(x_{i} y_{j}\right)=-\log \left[p\left(x_{i}\right) p\left(y_{j}\right)\right]=-\log p\left(x_{i}\right)-\log p\left(y_{j}\right)=I\left(x_{i}\right)+I\left(y_{j}\right)
$$
**条件自信息量定义为条件概率对数的负值。**设bj条件下，发生ai的条件概率为p(ai/bj)，那么它的条件自信息量l(ai/bj)定义为
$$
I\left(x_{i} | y_{j}\right)=-\log p\left(x_{i} | y_{j}\right)
$$
三者之间有这样的关系式：
$$
\begin{array}{l}
I\left(x_{i} y_{j}\right)=-\log p\left(x_{i}\right) p\left(y_{j} | x_{i}\right)=I\left(x_{i}\right)+I\left(y_{j} | x_{i}\right) \\
I\left(y_{j} x_{i}\right)=-\log p\left(y_{j}\right) p\left(x_{i} | y_{j}\right)=I\left(y_{j}\right)+I\left(x_{i} | y_{j}\right)
\end{array}
$$
**互信息量**：设有两个随机事件X 和Y, X 取值千信源发出的离散消息集合， Y 取值于信宿收到的离散消息集合。由千信宿事先不知道信源在某一时刻发出的是哪一个消息，所以每个消息是随机事件的一个可能结果。

考虑信源、信宿和有扰信道模型，设信源发送的信息为ai，信宿收到的信息为bj。

一般而言，信道中总是存在着噪声和干扰。信源发出的消息ai通过信道后，信宿只可能
收到由干扰作用引起的某种变型bj。信宿收到bJj后推测信源发出ai的概率，这一过程可由后验概率p(ai）来描述。相应地，信源发出消息ai的概率p(ai)称为先验概率。我们定义a; 的后验概率与先验概率比值的对数为bj对ai 的互信息量，也称交互信息量（简称互信息），用I(ai; bj)表示：
$$
I\left(a_{i} ; b_{j}\right)=\log _{2} \frac{p\left(a_{i} / b_{j}\right)}{p\left(a_{i}\right)} \quad(i=1,2, \cdots, n ; j=1,2, \cdots, m)
$$
表达式可简记为**log（先验概率/后验概率）**，也可以自信息量和条件自信息量的减法。具体使用可参考P12例2.1.2题做法。

互信息量的性质：互易性（互信息量交换变量不改变值），可为零（当两个时间完全独立时），可正可负（后验概率大于先验概率，互信息量大于零，否则小于零，<font color=red>互信息量大于零表示先验消息的出现有利于后验消息的出现</font>），互信息量不可能大于其中任一事件的自信息量。

**条件互信息量**：联合集XYZ中，在给定Zk的条件下，xi与yj之间的互信息量定义为条件互信息量。其定义为
$$
I\left(x_{i} ; y_{j} / z_{k}\right)=\log \frac{p\left(x_{i} / y_{j} z_{k}\right)}{p\left(x_{i} / z_{k}\right)}
$$
联合集XYZ上还存在xi与yjzk之间的互信息量，其定义式为：
$$
I\left(x_{i} ; y_{j} z_{k}\right)=\log \frac{p\left(x_{i} / y_{j} z_{k}\right)}{p\left(x_{i}\right)}
$$
不难看出它还有关系：
$$
I\left(x_{i} ; y_{j} z_{k}\right)=I\left(x_{i} ; y_{j}\right)+I\left(x_{i} ; z_{k} / y_{j}\right)
$$
总结：

o自信息量←→不确定度

o互信息量←→不确定度的减少量

o自信息量和互信息量，具有随机变量的性质，均为随机变量。

o自信息量只表示单个随机事件的不确定度，不能表示信源总体的不确定度。

### **<font color=red>信源熵</font>**

自信息量是一个随机变量它只表示了信源发出某一信息符号的不确定性，并不能反映整个信源的不确定性，并不能用作信源的信息测度。

```
与信源相关的结论：
信源的不确定程度与信源概率空间的状态数及其概率分布有关；
如果信源概率空间的状态数确定，概率分布为等概时，不确定程度最大；
等概时，不确定程度与信源概率空间的可能状态数（或相应的概率）有关，状态数越多（或相应的概率越小），不确定程度就越大。
```

我们定义信源各个离散消息的自信息量的数学期望（即概率加权的统计平均值）为信源的平均信息量，一般称为信源的信息墒，也叫信源墒或香农墒，有时称为无条件墒或炳函数，简称墒，记为H(X) 。
$$
H(X)=E\left[I\left(x_{i}\right)\right]=E\left[-\log p\left(x_{i}\right)\right]=-\sum_{i=1}^{n} p\left(x_{i}\right) \log p\left(x_{i}\right)
$$
熵=平均自信息量（数值相等）=加权平均

信息熵的单位取决于对数选取的底，对数选取的2为底则单位为bit/sign。

><font color=blue>信源熵与平均自信息量的区别</font>
>
>信源熵表征信源的平均不确定度，平均自信息量表征消除不确定度所需的信息的度量

```
信源熵的物理含义:
表示信源输出后，每个离散消息所提供的平均信息量；
表示信源输出前，信源的平均不确定度；
反映了变量X的随机性
```

### **条件熵**

条件墒是在联合符号集合XY 上的条件自信息量的数学期塑。在已知随机变量Y 的条件下，
随机变量X 的条件炳H(XIY) 定义为
$$
H(Y | X)=\sum_{X Y} p(x y) I(y | x)
$$
在给定的X 条件下， Y 的条件墒H(YIX) 为
$$
H(Y / X)=E\left[I\left(b_{j} / a_{i}\right)\right]=-\sum_{j=1}^{m} \sum_{i=1}^{n} p\left(a_{i} b_{j}\right) \log _{2} p\left(b_{j} / a_{i}\right)
$$
其中
$$
p\left(a_{i} / b_{j}\right)=\frac{p\left(a_{i} b_{j}\right)}{p\left(b_{j}\right)}
$$
条件炳是一个确定值，表示信宿在收到Y 后，信源X 仍然存在的不确定度。这是传输失真所造成的。有时我们称H(XIY)为信道疑义度，也称为损失墒。称条件墒H(YIX) 为噪声墒。

熵函数的性质：

1、对称性（概率矢量各分量的次序任意变更时熵值不变）说明信源熵仅与概率分布的统计特性有关

2、非负性，等于0意味着Pk=1

3、扩展性：
$$
\lim _{\varepsilon \rightarrow 0} H_{n+1}\left(p_{1}, p_{2}, \cdots, p_{n}-\varepsilon, \varepsilon\right)=H_{n}\left(p_{1}, p_{2}, \cdots, p_{n}\right)
$$
意味着一个事件的概率与其他事件相比很小的时候，它对集体熵值的贡献可以忽略

4、可加性，若两个随机变量并非独立，则有
$$
\begin{aligned}
&H(X Y)=H(X)+H(Y / X)\\
&H(X Y)=H(Y)+H(X / Y)
\end{aligned}
$$
若两个随机变量独立，则有
$$
H(X Y)=H(X)+H(Y)
$$
5、最大熵定理：离散情况下，集合中的事件依概率发生时，熵达到极大值
$$
H\left(p_{1}, p_{2}, \cdots, p_{n}\right) \leq H\left(\frac{1}{n}, \frac{1}{n}, \cdots, \frac{1}{n}\right)=\log n
$$
集合中元素越多，其熵值也就越大

6、确定性
$$
H\left(p_{1}, p_{2}, \cdots, p_{n}\right) \leq H\left(\frac{1}{n}, \frac{1}{n}, \cdots, \frac{1}{n}\right)=\log n
$$
集合X中只要有一个事件为必然事件，则其余事件为不可能事件。此时，集合X中每个事件对熵的贡献都为零，因而熵必为零。

7、极值性
$$
H_{n}\left(p\left(x_{1}\right), p\left(x_{2}\right), \cdots p\left(x_{n}\right)\right) \leq-\sum_{i=1}^{n} p\left(x_{i}\right) \log _{2} p\left(y_{i}\right)
$$
8、上凸函数性质（上凸函数任意两点画一条割线，函数总在割线的上方）

### **联合熵、条件熵和信息熵的关系**

$$
\mathrm{H}(\mathrm{X}, \mathrm{Y})=\mathrm{H}(\mathrm{X})+\mathrm{H}(\mathrm{Y} / \mathrm{X}) ; \quad \mathrm{H}(\mathrm{Y}, \mathrm{X})=\mathrm{H}(\mathrm{Y})+\mathrm{H}(\mathrm{X} / \mathrm{Y})
$$

$$
\begin{aligned}
&\mathrm{H}(\mathrm{X})+\mathrm{H}(\mathrm{Y} / \mathrm{X})=\mathrm{H}(\mathrm{Y})+\mathrm{H}(\mathrm{X} / \mathrm{Y})\\
&\mathrm{H}(\mathrm{X})-\mathrm{H}(\mathrm{X} | \mathrm{Y})=\mathrm{H}(\mathrm{Y})-\mathrm{H}(\mathrm{Y} | \mathrm{X})
\end{aligned}
$$

若XY相互独立则有
$$
\mathrm{H}(\mathrm{X}, \mathrm{Y})=\mathrm{H}(\mathrm{X})+\mathrm{H}(\mathrm{Y})
$$
推广到N个随机变量：
$$
\begin{aligned}
&H\left(X_{1}, X_{2}, \cdots, X_{N}\right)=H\left(X_{1}\right)+H\left(X_{2} | X_{1}\right)+\cdots+H_{N}\left(X_{N} | X_{1} X_{2} \cdots X_{N-1}\right)\\
&H\left(X_{1}, X_{2}, \cdots, X_{N}\right)=\sum_{i=1}^{N} H\left(X_{i} | X_{1} X_{2} \cdots X_{i-1}\right)
\end{aligned}
$$
同理：
$$
H\left(X_{1}, X_{2}, \cdots, X_{N}\right)=\sum_{i=1}^{N} H\left(X_{i}\right)
$$
### **共熵与信息熵的关系**

$$
H(X, Y) \leq H(X)+H(Y)
$$
当集合X和Y取自同一集合Z时，则有
$$
H(X)=H(Y)=H(Z) \quad \text { A } \quad H(X, Y) \leq 2 H(X)
$$
该性质可拓展。

**条件熵与信源熵的关系**：条件熵小于信源熵，H(X/Y)≤H(X)

**离散集的平均互信息量**：输入输出均抽象成两个离散集，且存在一定的概率关系。o互信息量I(xi;yj)表征的是收到yj后得到的有关xi的信息量（“输入xi ，输出yj”是一个概率为p(xi yj) 的随机事件，相应的I(xi;yj)也是随xi和yj变化而变化的随机量）。

互信息量I(xi;yj)不能从整体上作为信道中信息流通的测度。

**平均互信息量定义**：互信息量I(xi;yj)在联合概率空间P(XY)中的统计平均值
$$
I(X ; Y)=\sum_{i=1}^{n} \sum_{j=1}^{m} p\left(x_{i} y_{j}\right) I\left(x_{i} ; y_{j}\right)=\sum_{i=1}^{n} \sum_{j=1}^{m} p\left(x_{i} y_{j}\right) \log \frac{p\left(x_{i} / y_{j}\right)}{p\left(x_{i}\right)}
$$
平均互信息量的其他表示：
$$
\begin{aligned}
&I(X ; Y)=\sum_{X Y} p\left(x_{i} y_{j}\right) \log \frac{p\left(x_{i} | y_{j}\right)}{p\left(x_{i}\right)}\\
&I(X ; Y)=\sum_{\lambda \gamma} p\left(x_{i}\right) p\left(y_{j} | x_{i}\right) \log \frac{p\left(y_{j} | x_{i}\right)}{p\left(y_{j}\right)}\\
&I(X ; Y)=\sum_{X Y} p\left(x_{i} y_{j}\right) I\left(x_{i} ; y_{j}\right)\\
& I\left(x_{i} ; y_{j}\right)=\log \frac{p\left(x_{i} | y_{j}\right)}{p\left(x_{i}\right)}=\log \frac{p\left(y_{j} | x_{i}\right)}{p\left(y_{j}\right)}
\end{aligned}
$$

> 平均互信息量的性质：
>
> 1、互易性（对称性）
>
> 2、非负性
>
> 3、极值性
>
> 4、凸函数性
>
> 基本类似熵的性质

平均互信息量和熵之间的关系：
$$
\begin{aligned}
&\begin{array}{l}
\mathrm{I}(\mathrm{X} ; \mathrm{Y})=\mathrm{H}(\mathrm{X})-\mathrm{H}(\mathrm{X} / \mathrm{Y}) \\
\mathrm{I}(\mathrm{X} ; \mathrm{Y})=\mathrm{H}(\mathrm{Y})-\mathrm{H}(\mathrm{Y} / \mathrm{X})
\end{array}\\
&\mathrm{I}(\mathrm{X} ; \mathrm{Y})=\mathrm{H}(\mathrm{X})+\mathrm{H}(\mathrm{Y})-\mathrm{H}(\mathrm{XY})
\end{aligned}
$$
可以参考维拉图

![维拉图](http://yanxuan.nosdn.127.net/d2f90c63eae022e663d1875ff7fde2d1.png)

### 多符号离散平稳信源

二进制信源的N次扩展信源：

![UTOOLS1587539098837.png](http://yanxuan.nosdn.127.net/f918108947ede4d777428128e876158b.png)

N次拓展信源共有2的N次方个符号，符号长度为N。

**N次拓展信源的熵：N次拓展信源的熵是未拓展信源的N倍**

证明：

![UTOOLS1587608985982.png](http://yanxuan.nosdn.127.net/40c4c144916c880dfaa1b8012649c6fe.png)



**平稳信源**：实际信源常不是简单无记忆信源，而是空间或时间的离散随机序列，常用联合概率来描述符号间的相互依存关系，为此引入平稳信源概念

![UTOOLS1587610740967.png](http://yanxuan.nosdn.127.net/e5e85cb3f5677b5267e0d9170ad58fb4.png)

**平稳意味着联合分布概率与时间起点无关**

离散平稳信源一般是有记忆信源。

平稳信源的熵：联合熵。

![UTOOLS1587611271360.png](http://yanxuan.nosdn.127.net/159e4d35de411fc979153db6602e8ec1.png)

熵的可加性的一般公式：
$$
H\left(X_{1}, X_{2}\right)=H\left(X_{1}\right)+H\left(X_{2} | X_{1}\right)
$$
以及二者独立时的公式：
$$
H\left(X_{1}, X_{2}\right)=H\left(X_{1}\right)+H\left(X_{2}\right)
$$
**离散无记忆信源的二次扩展信源**可看成是二维离散平稳信源的**特例**。

![UTOOLS1587611604478.png](http://yanxuan.nosdn.127.net/2a093d520d744ffc0f1c3d818f3ddcd7.png)

**二维平稳信源与二次扩展信源的熵**

![UTOOLS1587612325594.png](http://yanxuan.nosdn.127.net/788a441fd6468b11dcff9f415022cd1a.png)

**极限熵**

![UTOOLS1587628496615.png](http://yanxuan.nosdn.127.net/a2c672bcf0134334395448f5cbb8849b.png)

**条件熵随N的增加而递减**
$$
H\left(X_{N} | X_{1}, \ldots, X_{N-1}\right) \leq H\left(X_{N-1} | X_{1}, \ldots, X_{N-2}\right) \leq \ldots \leq H\left(X_{2} | X_{1}\right) \leq H\left(X_{1}\right)
$$
**平均符号熵**

![UTOOLS1587628847498.png](http://yanxuan.nosdn.127.net/398c634ce036b834b2337d95cbb93377.png)

**极限熵存在定理**：

![UTOOLS1587628957648.png](http://yanxuan.nosdn.127.net/4ac87e9c5f6052b0ff171d95f3756ba9.png)

**马尔可夫信源：**

有记忆信源在任一时刻发出符号的概率通常仅与前面的若干个符号有关，与更前面的符号无关，因此我们可以认为信源在某一时刻发出的符号与信源的状态有关。

![UTOOLS1587629182038.png](http://yanxuan.nosdn.127.net/a3c273b317f1cd109a1e5b27ff80f5da.png)

```
马尔可夫信源定义：信源输出的符号序列和状态序列满足如下条件称为马尔可夫信源
1、某一时刻信源的输出近于信源当前的状态有关
2、信源的状态只与当前的输出符号和前一时刻信源状态唯一确定
```

马尔可夫信源输出的符号序列**X**完全由信源所处的状态**S**决定。

可将信源的输出**符号系列**变换成**状态系列**，将信源输出符号的不确定性问题变成信源状态的转换问题。

马尔可夫信源的极限熵：
$$
\begin{aligned}
H_{\infty}=& \lim _{N \rightarrow \infty} H\left(X_{N} | X_{1}, X_{2}, \cdots, X_{N-1}\right)=\\
& H\left(X_{m+1} | X_{1}, X_{2}, \cdots, X_{m}\right)=H_{m+1}
\end{aligned}
$$
即m阶马尔可夫信源的极限熵等于m阶条件熵，而Hm+1的计算：
$$
\begin{array}{l}
p\left(x_{k_{m+1}} | x_{k_{m}}, \cdots, x_{k_{1}}\right)=p\left(e_{j} | e_{i}\right) \\
H\left(x_{k_{m+1}} | x_{k_{m}}, \cdots, x_{k_{1}}\right)=H_{m+1} \\
\quad=-\sum_{k_{m+1}, \cdots, k_{1}} p\left(x_{k_{m+1}}, \cdots, x_{k_{1}}\right) \bullet \log p\left(x_{k_{m+1}} | x_{k_{m}}, \cdots, x_{k_{1}}\right) \\
\quad=-\sum_{i} \sum_{j} p\left(e_{i}\right) p\left(e_{j} | e_{i}\right) \log p\left(e_{j} | e_{i}\right)
\end{array}
$$
信源输出符号间的依赖关系即相关性使得信源熵减小，相关程度越大，信源熵越小，越趋于极限熵。

信源的相关性和剩余度：
$$
H\left(\mathrm{X}_{\mathrm{N}} | X_{1}, X_{2}, \ldots, X_{N-1}\right) \leq H\left(\mathrm{X}_{\mathrm{N}-1} | X_{1}, X_{2}, \ldots, X_{N-2}\right) \leq \cdots \leq H\left(\mathrm{X}_{2} | X_{1}\right) \leq H\left(\mathrm{X}_{1}\right)
$$
当离散平稳信源输出符号为等概率分布时熵最大，即平均自信息量
$$
H_{0}=\log q
$$
于是有：
$$
H_{0} \geq H_{1} \geq H_{2} \geq \cdots \geq H_{m+1} \geq \cdots \geq H_{\infty}
$$

```
由此看出，信源输出符号间的依赖关系即相关性使得信源熵减小
相关程度越大，信源熵越小，趋于极限熵
相关程度减小，信源熵越大当符号间彼此不相关且呈等概率分布时，熵最大
```

信源剩余度：衡量信源相关性程度，引入剩余度概念。定义：
$$
R=1-\frac{H_{\infty}}{H_{0}}
$$
其中H∞为信源实际熵，H0=Hmax为信源最大熵。称：
$$
\eta=\frac{H_{\infty}}{H_{0}}
$$
为信源熵的相对率，有R=1-η

可见信源输出符号间相关长度长，则信源的实际熵小，熵的相对率小，信源的剩余度越大；反之，信源剩余度小

### 连续信源

对于某一连续信源X(t),当给定某一时刻t=to时，其取值是连续的，即时间和幅度均为连续函数。由于连续信源中消息数是无限的，其每一可能的消息是随机过程的一个样本函数，可以用有限维概率分布函数或有限维概率密度函数来描述连续信源。

**数学模型**

![UTOOLS1588039961928.png](http://yanxuan.nosdn.127.net/4ad98dbc479e851fd610acac820d4cdc.png)

任何一个随机过程都可用一组随机变量来描述，研究连续信源可以首先对单个随机变量情况进行讨论，然后推广到n维情况。

**连续信源的熵**

简单连续信源的模型写作：

![UTOOLS1588047119572.png](http://yanxuan.nosdn.127.net/4cb1b48fecf3570c3b777e0c79ca96b3.png)

可以改写成离散信源：

![UTOOLS1588047345999.png](http://yanxuan.nosdn.127.net/76fe90aa329cd2ec72571f62e544ecd8.png)

由积分中值定理得到：
$$
p_{i}=\int_{a+(i-1) \Delta x}^{a+i \Delta x} p(x) d x=p\left(x_{i}\right) \Delta x
$$
离散化后，对连续信源的熵，可以用此离散信源的熵来近似。

定义连续信源的熵：
$$
H(X)=-\int_{-\infty}^{\infty} p(x) \log p(x) d x
$$

```
1、连续信源的熵与离散信源的熵具有相同的形式，但其意义不相同。连续信源熵与离散信源熵相比，去掉了一个无穷项，连续信源的不确定性应为无穷大。
2、由于实际应用中常常关心的是熵之间的差值，无穷项可相互抵消，故这样定义连续信源的熵不会影响讨论所关心的交互信息量、信息容量和率失真函数。
3、需要强调的是连续信源熵的值只是熵的相对值，不是绝对值，而离散信源熵的值是绝对值。
```

**连续信源的最大熵**







## 参考资料

> - 信息论课程PPT——[提取码：pvr0](https://pan.baidu.com/s/1L2Qoz7n8ybcdvTRrG2RAqw 
>   )
> - [阮一峰的信息论入门](https://www.ruanyifeng.com/blog/2019/08/information-theory.html)
> - []()
> - []()
